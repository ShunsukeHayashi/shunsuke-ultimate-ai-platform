"""
Ultimate ShunsukeModel Ecosystem - Quality Guardian
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

ã‚³ãƒ¼ãƒ‰å“è³ªã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å“è³ªã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å“è³ªã‚’ç¶™ç¶šçš„ã«ç›£è¦–ã—ã€
è‡ªå‹•æ”¹å–„ææ¡ˆã‚’è¡Œã†ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆå“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional, Set, Callable
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import json
import yaml
from datetime import datetime, timezone, timedelta
import ast
import re
import subprocess
import hashlib
import statistics

from ...orchestration.coordinator.agent_coordinator import AgentCoordinator
from ...orchestration.communication.communication_protocol import CommunicationProtocol, MessageType


class QualityMetric(Enum):
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    CODE_COMPLEXITY = "code_complexity"
    TEST_COVERAGE = "test_coverage"
    DOCUMENTATION = "documentation"
    PERFORMANCE = "performance"
    SECURITY = "security"
    MAINTAINABILITY = "maintainability"
    RELIABILITY = "reliability"
    USABILITY = "usability"
    PORTABILITY = "portability"
    SCALABILITY = "scalability"


class QualitySeverity(Enum):
    """å“è³ªå•é¡Œã®é‡è¦åº¦"""
    CRITICAL = "critical"  # å³åº§ã«ä¿®æ­£ãŒå¿…è¦
    HIGH = "high"  # é«˜å„ªå…ˆåº¦
    MEDIUM = "medium"  # ä¸­å„ªå…ˆåº¦
    LOW = "low"  # ä½å„ªå…ˆåº¦
    INFO = "info"  # æƒ…å ±ãƒ¬ãƒ™ãƒ«


class QualityCheckType(Enum):
    """å“è³ªãƒã‚§ãƒƒã‚¯ã‚¿ã‚¤ãƒ—"""
    STATIC_ANALYSIS = "static_analysis"
    DYNAMIC_ANALYSIS = "dynamic_analysis"
    ARCHITECTURAL = "architectural"
    PERFORMANCE = "performance"
    SECURITY = "security"
    INTEGRATION = "integration"


@dataclass
class QualityIssue:
    """å“è³ªå•é¡Œ"""
    id: str
    metric: QualityMetric
    severity: QualitySeverity
    title: str
    description: str
    file_path: Optional[str] = None
    line_number: Optional[int] = None
    column_number: Optional[int] = None
    rule_id: Optional[str] = None
    suggestion: Optional[str] = None
    auto_fixable: bool = False
    detected_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class QualityReport:
    """å“è³ªãƒ¬ãƒãƒ¼ãƒˆ"""
    id: str
    project_path: str
    timestamp: datetime
    overall_score: float  # 0.0 - 1.0
    metric_scores: Dict[QualityMetric, float] = field(default_factory=dict)
    issues: List[QualityIssue] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    trends: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def critical_issues(self) -> List[QualityIssue]:
        """ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å•é¡Œã®ãƒªã‚¹ãƒˆ"""
        return [issue for issue in self.issues if issue.severity == QualitySeverity.CRITICAL]
    
    @property
    def high_priority_issues(self) -> List[QualityIssue]:
        """é«˜å„ªå…ˆåº¦å•é¡Œã®ãƒªã‚¹ãƒˆ"""
        return [issue for issue in self.issues if issue.severity == QualitySeverity.HIGH]
    
    @property
    def auto_fixable_issues(self) -> List[QualityIssue]:
        """è‡ªå‹•ä¿®æ­£å¯èƒ½å•é¡Œã®ãƒªã‚¹ãƒˆ"""
        return [issue for issue in self.issues if issue.auto_fixable]


@dataclass
class QualityThreshold:
    """å“è³ªé–¾å€¤"""
    metric: QualityMetric
    min_acceptable: float
    target: float
    critical_threshold: float
    enabled: bool = True


class QualityAnalyzer:
    """å“è³ªåˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.analyzers: Dict[QualityCheckType, Callable] = {}
        self.logger = logging.getLogger(__name__)
        self._setup_analyzers()
    
    def _setup_analyzers(self):
        """åˆ†ææ©Ÿèƒ½ã®è¨­å®š"""
        self.analyzers[QualityCheckType.STATIC_ANALYSIS] = self._static_analysis
        self.analyzers[QualityCheckType.DYNAMIC_ANALYSIS] = self._dynamic_analysis
        self.analyzers[QualityCheckType.ARCHITECTURAL] = self._architectural_analysis
        self.analyzers[QualityCheckType.PERFORMANCE] = self._performance_analysis
        self.analyzers[QualityCheckType.SECURITY] = self._security_analysis
    
    async def analyze_project(self, project_path: Path) -> QualityReport:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®å“è³ªåˆ†æ"""
        report_id = hashlib.md5(f"{project_path}_{datetime.now().isoformat()}".encode()).hexdigest()[:8]
        
        report = QualityReport(
            id=report_id,
            project_path=str(project_path),
            timestamp=datetime.now(timezone.utc),
            overall_score=0.0
        )
        
        # å„ãƒã‚§ãƒƒã‚¯ã‚¿ã‚¤ãƒ—ã‚’å®Ÿè¡Œ
        for check_type, analyzer in self.analyzers.items():
            try:
                issues = await analyzer(project_path)
                report.issues.extend(issues)
                self.logger.debug(f"{check_type.value} found {len(issues)} issues")
            except Exception as e:
                self.logger.error(f"Analysis failed for {check_type.value}: {e}")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—
        report.metric_scores = await self._calculate_metric_scores(project_path, report.issues)
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        report.overall_score = self._calculate_overall_score(report.metric_scores)
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        report.recommendations = await self._generate_recommendations(report)
        
        return report
    
    async def _static_analysis(self, project_path: Path) -> List[QualityIssue]:
        """é™çš„è§£æ"""
        issues = []
        
        # Python ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ
        python_files = list(project_path.rglob("*.py"))
        
        for py_file in python_files:
            try:
                file_issues = await self._analyze_python_file(py_file)
                issues.extend(file_issues)
            except Exception as e:
                self.logger.warning(f"Failed to analyze {py_file}: {e}")
        
        return issues
    
    async def _analyze_python_file(self, file_path: Path) -> List[QualityIssue]:
        """Python ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ"""
        issues = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                tree = ast.parse(content)
            
            # è¤‡é›‘åº¦ãƒã‚§ãƒƒã‚¯
            complexity_issues = self._check_complexity(tree, file_path)
            issues.extend(complexity_issues)
            
            # ã‚³ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
            style_issues = await self._check_code_style(file_path, content)
            issues.extend(style_issues)
            
            # å‘½åè¦å‰‡ãƒã‚§ãƒƒã‚¯
            naming_issues = self._check_naming_conventions(tree, file_path)
            issues.extend(naming_issues)
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
            doc_issues = self._check_documentation(tree, file_path)
            issues.extend(doc_issues)
            
        except SyntaxError as e:
            issues.append(QualityIssue(
                id=f"syntax_error_{hash(str(file_path))}",
                metric=QualityMetric.CODE_COMPLEXITY,
                severity=QualitySeverity.CRITICAL,
                title="Syntax Error",
                description=f"Syntax error: {e.msg}",
                file_path=str(file_path),
                line_number=e.lineno,
                column_number=e.offset
            ))
        
        return issues
    
    def _check_complexity(self, tree: ast.AST, file_path: Path) -> List[QualityIssue]:
        """è¤‡é›‘åº¦ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        class ComplexityChecker(ast.NodeVisitor):
            def __init__(self):
                self.complexity = 1  # ãƒ™ãƒ¼ã‚¹è¤‡é›‘åº¦
                self.function_complexities = {}
                self.current_function = None
            
            def visit_FunctionDef(self, node):
                old_function = self.current_function
                old_complexity = self.complexity
                
                self.current_function = node.name
                self.complexity = 1
                
                self.generic_visit(node)
                
                self.function_complexities[node.name] = {
                    'complexity': self.complexity,
                    'line': node.lineno
                }
                
                self.current_function = old_function
                self.complexity = old_complexity
            
            def visit_If(self, node):
                self.complexity += 1
                self.generic_visit(node)
            
            def visit_While(self, node):
                self.complexity += 1
                self.generic_visit(node)
            
            def visit_For(self, node):
                self.complexity += 1
                self.generic_visit(node)
            
            def visit_Try(self, node):
                self.complexity += 1
                self.generic_visit(node)
        
        checker = ComplexityChecker()
        checker.visit(tree)
        
        # é«˜è¤‡é›‘åº¦é–¢æ•°ã‚’ãƒ¬ãƒãƒ¼ãƒˆ
        for func_name, data in checker.function_complexities.items():
            if data['complexity'] > 10:  # é–¾å€¤
                severity = QualitySeverity.HIGH if data['complexity'] > 15 else QualitySeverity.MEDIUM
                
                issues.append(QualityIssue(
                    id=f"complexity_{hash(f'{file_path}_{func_name}')}",
                    metric=QualityMetric.CODE_COMPLEXITY,
                    severity=severity,
                    title=f"High Complexity Function: {func_name}",
                    description=f"Function '{func_name}' has cyclomatic complexity of {data['complexity']}",
                    file_path=str(file_path),
                    line_number=data['line'],
                    suggestion="Consider breaking this function into smaller functions"
                ))
        
        return issues
    
    async def _check_code_style(self, file_path: Path, content: str) -> List[QualityIssue]:
        """ã‚³ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        lines = content.split('\n')
        
        for i, line in enumerate(lines, 1):
            # é•·ã„è¡Œã‚’ãƒã‚§ãƒƒã‚¯
            if len(line) > 120:
                issues.append(QualityIssue(
                    id=f"long_line_{hash(f'{file_path}_{i}')}",
                    metric=QualityMetric.MAINTAINABILITY,
                    severity=QualitySeverity.LOW,
                    title="Line Too Long",
                    description=f"Line {i} is {len(line)} characters long (max 120)",
                    file_path=str(file_path),
                    line_number=i,
                    auto_fixable=True
                ))
            
            # ã‚¿ãƒ–æ–‡å­—ã‚’ãƒã‚§ãƒƒã‚¯
            if '\t' in line:
                issues.append(QualityIssue(
                    id=f"tab_char_{hash(f'{file_path}_{i}')}",
                    metric=QualityMetric.MAINTAINABILITY,
                    severity=QualitySeverity.LOW,
                    title="Tab Characters Found",
                    description=f"Line {i} contains tab characters (use spaces)",
                    file_path=str(file_path),
                    line_number=i,
                    auto_fixable=True
                ))
        
        return issues
    
    def _check_naming_conventions(self, tree: ast.AST, file_path: Path) -> List[QualityIssue]:
        """å‘½åè¦å‰‡ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        class NamingChecker(ast.NodeVisitor):
            def visit_FunctionDef(self, node):
                # snake_case ãƒã‚§ãƒƒã‚¯
                if not re.match(r'^[a-z_][a-z0-9_]*$', node.name):
                    issues.append(QualityIssue(
                        id=f"naming_func_{hash(f'{file_path}_{node.name}')}",
                        metric=QualityMetric.MAINTAINABILITY,
                        severity=QualitySeverity.LOW,
                        title="Function Naming Convention",
                        description=f"Function '{node.name}' should use snake_case",
                        file_path=str(file_path),
                        line_number=node.lineno
                    ))
                
                self.generic_visit(node)
            
            def visit_ClassDef(self, node):
                # PascalCase ãƒã‚§ãƒƒã‚¯
                if not re.match(r'^[A-Z][a-zA-Z0-9]*$', node.name):
                    issues.append(QualityIssue(
                        id=f"naming_class_{hash(f'{file_path}_{node.name}')}",
                        metric=QualityMetric.MAINTAINABILITY,
                        severity=QualitySeverity.LOW,
                        title="Class Naming Convention",
                        description=f"Class '{node.name}' should use PascalCase",
                        file_path=str(file_path),
                        line_number=node.lineno
                    ))
                
                self.generic_visit(node)
        
        checker = NamingChecker()
        checker.visit(tree)
        
        return issues
    
    def _check_documentation(self, tree: ast.AST, file_path: Path) -> List[QualityIssue]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        class DocChecker(ast.NodeVisitor):
            def visit_FunctionDef(self, node):
                # ãƒ‘ãƒ–ãƒªãƒƒã‚¯é–¢æ•°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯
                if not node.name.startswith('_'):  # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆé–¢æ•°ã§ãªã„
                    docstring = ast.get_docstring(node)
                    if not docstring:
                        issues.append(QualityIssue(
                            id=f"missing_doc_{hash(f'{file_path}_{node.name}')}",
                            metric=QualityMetric.DOCUMENTATION,
                            severity=QualitySeverity.MEDIUM,
                            title="Missing Docstring",
                            description=f"Function '{node.name}' is missing a docstring",
                            file_path=str(file_path),
                            line_number=node.lineno,
                            suggestion="Add a docstring describing the function's purpose, parameters, and return value"
                        ))
                
                self.generic_visit(node)
            
            def visit_ClassDef(self, node):
                docstring = ast.get_docstring(node)
                if not docstring:
                    issues.append(QualityIssue(
                        id=f"missing_class_doc_{hash(f'{file_path}_{node.name}')}",
                        metric=QualityMetric.DOCUMENTATION,
                        severity=QualitySeverity.MEDIUM,
                        title="Missing Class Docstring",
                        description=f"Class '{node.name}' is missing a docstring",
                        file_path=str(file_path),
                        line_number=node.lineno,
                        suggestion="Add a docstring describing the class's purpose and usage"
                    ))
                
                self.generic_visit(node)
        
        checker = DocChecker()
        checker.visit(tree)
        
        return issues
    
    async def _dynamic_analysis(self, project_path: Path) -> List[QualityIssue]:
        """å‹•çš„è§£æ"""
        issues = []
        
        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯
        coverage_issues = await self._check_test_coverage(project_path)
        issues.extend(coverage_issues)
        
        return issues
    
    async def _check_test_coverage(self, project_path: Path) -> List[QualityIssue]:
        """ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        try:
            # pytest-cov ã‚’ä½¿ç”¨ã—ã¦ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’æ¸¬å®š
            result = subprocess.run(
                ['python', '-m', 'pytest', '--cov=.', '--cov-report=json', '--cov-report=term-missing'],
                cwd=project_path,
                capture_output=True,
                text=True,
                timeout=60
            )
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            coverage_file = project_path / '.coverage'
            if coverage_file.exists():
                # ç°¡æ˜“çš„ãªã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯è©³ç´°ãªè§£æãŒå¿…è¦ï¼‰
                issues.append(QualityIssue(
                    id=f"coverage_check_{hash(str(project_path))}",
                    metric=QualityMetric.TEST_COVERAGE,
                    severity=QualitySeverity.INFO,
                    title="Test Coverage Analysis",
                    description="Test coverage analysis completed",
                    metadata={"coverage_available": True}
                ))
        
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
            issues.append(QualityIssue(
                id=f"no_coverage_{hash(str(project_path))}",
                metric=QualityMetric.TEST_COVERAGE,
                severity=QualitySeverity.MEDIUM,
                title="No Test Coverage",
                description="Unable to determine test coverage",
                suggestion="Set up pytest-cov for test coverage analysis"
            ))
        
        return issues
    
    async def _architectural_analysis(self, project_path: Path) -> List[QualityIssue]:
        """ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æ"""
        issues = []
        
        # ä¾å­˜é–¢ä¿‚åˆ†æ
        dependency_issues = await self._analyze_dependencies(project_path)
        issues.extend(dependency_issues)
        
        return issues
    
    async def _analyze_dependencies(self, project_path: Path) -> List[QualityIssue]:
        """ä¾å­˜é–¢ä¿‚åˆ†æ"""
        issues = []
        
        requirements_file = project_path / 'requirements.txt'
        if requirements_file.exists():
            try:
                with open(requirements_file, 'r') as f:
                    requirements = f.read().splitlines()
                
                # ãƒãƒ¼ã‚¸ãƒ§ãƒ³å›ºå®šã®ãªã„ä¾å­˜é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯
                for req in requirements:
                    if req.strip() and not any(op in req for op in ['==', '>=', '<=', '>', '<', '~=', '!=']):
                        issues.append(QualityIssue(
                            id=f"unpinned_dep_{hash(req)}",
                            metric=QualityMetric.RELIABILITY,
                            severity=QualitySeverity.MEDIUM,
                            title="Unpinned Dependency",
                            description=f"Dependency '{req}' should specify a version",
                            file_path=str(requirements_file),
                            suggestion="Pin dependency versions for reproducible builds"
                        ))
            
            except Exception as e:
                self.logger.warning(f"Failed to analyze requirements.txt: {e}")
        
        return issues
    
    async def _performance_analysis(self, project_path: Path) -> List[QualityIssue]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        issues = []
        
        # æ½œåœ¨çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã‚’ãƒã‚§ãƒƒã‚¯
        python_files = list(project_path.rglob("*.py"))
        
        for py_file in python_files:
            try:
                perf_issues = await self._analyze_performance_patterns(py_file)
                issues.extend(perf_issues)
            except Exception as e:
                self.logger.warning(f"Performance analysis failed for {py_file}: {e}")
        
        return issues
    
    async def _analyze_performance_patterns(self, file_path: Path) -> List[QualityIssue]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        issues = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # éåŠ¹ç‡ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
            patterns = [
                (r'for\s+\w+\s+in\s+range\(len\([^)]+\)\):', 
                 "Use enumerate() instead of range(len())",
                 "performance_range_len"),
                (r'\.keys\(\)\s*in\s+', 
                 "Checking membership in dict.keys() is inefficient, use 'in dict' directly",
                 "performance_keys_membership"),
                (r'list\([^)]*\.keys\(\)\)', 
                 "Converting dict.keys() to list is often unnecessary",
                 "performance_list_keys")
            ]
            
            lines = content.split('\n')
            for i, line in enumerate(lines, 1):
                for pattern, suggestion, rule_id in patterns:
                    if re.search(pattern, line):
                        issues.append(QualityIssue(
                            id=f"{rule_id}_{hash(f'{file_path}_{i}')}",
                            metric=QualityMetric.PERFORMANCE,
                            severity=QualitySeverity.LOW,
                            title="Performance Anti-pattern",
                            description=f"Line {i}: {suggestion}",
                            file_path=str(file_path),
                            line_number=i,
                            rule_id=rule_id,
                            suggestion=suggestion
                        ))
        
        except Exception as e:
            self.logger.warning(f"Failed to analyze performance patterns in {file_path}: {e}")
        
        return issues
    
    async def _security_analysis(self, project_path: Path) -> List[QualityIssue]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åˆ†æ"""
        issues = []
        
        python_files = list(project_path.rglob("*.py"))
        
        for py_file in python_files:
            try:
                security_issues = await self._analyze_security_patterns(py_file)
                issues.extend(security_issues)
            except Exception as e:
                self.logger.warning(f"Security analysis failed for {py_file}: {e}")
        
        return issues
    
    async def _analyze_security_patterns(self, file_path: Path) -> List[QualityIssue]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        issues = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ã®ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
            security_patterns = [
                (r'eval\s*\(', 
                 "Use of eval() can be dangerous - consider safer alternatives",
                 QualitySeverity.HIGH,
                 "security_eval"),
                (r'exec\s*\(', 
                 "Use of exec() can be dangerous - consider safer alternatives",
                 QualitySeverity.HIGH,
                 "security_exec"),
                (r'subprocess\.call\([^)]*shell\s*=\s*True', 
                 "subprocess with shell=True can be vulnerable to injection attacks",
                 QualitySeverity.MEDIUM,
                 "security_shell_injection"),
                (r'password\s*=\s*["\'][^"\']+["\']', 
                 "Hardcoded password detected",
                 QualitySeverity.CRITICAL,
                 "security_hardcoded_password"),
                (r'secret\s*=\s*["\'][^"\']+["\']', 
                 "Hardcoded secret detected",
                 QualitySeverity.CRITICAL,
                 "security_hardcoded_secret")
            ]
            
            lines = content.split('\n')
            for i, line in enumerate(lines, 1):
                for pattern, description, severity, rule_id in security_patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        issues.append(QualityIssue(
                            id=f"{rule_id}_{hash(f'{file_path}_{i}')}",
                            metric=QualityMetric.SECURITY,
                            severity=severity,
                            title="Security Risk",
                            description=f"Line {i}: {description}",
                            file_path=str(file_path),
                            line_number=i,
                            rule_id=rule_id,
                            suggestion=description
                        ))
        
        except Exception as e:
            self.logger.warning(f"Failed to analyze security patterns in {file_path}: {e}")
        
        return issues
    
    async def _calculate_metric_scores(self, project_path: Path, issues: List[QualityIssue]) -> Dict[QualityMetric, float]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        scores = {}
        
        # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å•é¡Œã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        issue_counts = {}
        for issue in issues:
            metric = issue.metric
            if metric not in issue_counts:
                issue_counts[metric] = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0, 'info': 0}
            issue_counts[metric][issue.severity.value] += 1
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ1.0ãŒæœ€é«˜ã€0.0ãŒæœ€ä½ï¼‰
        for metric in QualityMetric:
            if metric in issue_counts:
                counts = issue_counts[metric]
                # é‡ã¿ä»˜ããƒšãƒŠãƒ«ãƒ†ã‚£è¨ˆç®—
                penalty = (counts['critical'] * 0.5 + 
                          counts['high'] * 0.3 + 
                          counts['medium'] * 0.1 + 
                          counts['low'] * 0.05)
                
                # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå•é¡Œæ•°ã«åŸºã¥ãæ¸›ç‚¹æ–¹å¼ï¼‰
                base_score = 1.0
                scores[metric] = max(0.0, base_score - penalty / 10)  # æœ€å¤§10å•é¡Œã§0ç‚¹
            else:
                scores[metric] = 1.0  # å•é¡ŒãŒãªã„å ´åˆã¯æº€ç‚¹
        
        return scores
    
    def _calculate_overall_score(self, metric_scores: Dict[QualityMetric, float]) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not metric_scores:
            return 0.0
        
        # é‡ã¿ä»˜ãå¹³å‡ã‚’è¨ˆç®—
        weights = {
            QualityMetric.SECURITY: 0.25,
            QualityMetric.RELIABILITY: 0.20,
            QualityMetric.MAINTAINABILITY: 0.15,
            QualityMetric.CODE_COMPLEXITY: 0.15,
            QualityMetric.TEST_COVERAGE: 0.10,
            QualityMetric.PERFORMANCE: 0.10,
            QualityMetric.DOCUMENTATION: 0.05
        }
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for metric, score in metric_scores.items():
            weight = weights.get(metric, 0.01)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡ã¿
            weighted_sum += score * weight
            total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.0
    
    async def _generate_recommendations(self, report: QualityReport) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å•é¡Œã¸ã®å¯¾å¿œ
        if report.critical_issues:
            recommendations.append(f"ğŸš¨ {len(report.critical_issues)} critical issues require immediate attention")
        
        # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨äº‹é …
        for metric, score in report.metric_scores.items():
            if score < 0.6:  # 60%æœªæº€
                if metric == QualityMetric.TEST_COVERAGE:
                    recommendations.append("ğŸ“‹ Improve test coverage by adding more unit tests")
                elif metric == QualityMetric.DOCUMENTATION:
                    recommendations.append("ğŸ“š Add docstrings and comments to improve code documentation")
                elif metric == QualityMetric.CODE_COMPLEXITY:
                    recommendations.append("ğŸ”§ Refactor complex functions to improve maintainability")
                elif metric == QualityMetric.SECURITY:
                    recommendations.append("ğŸ”’ Address security vulnerabilities immediately")
        
        # è‡ªå‹•ä¿®æ­£å¯èƒ½ãªå•é¡Œ
        auto_fixable_count = len(report.auto_fixable_issues)
        if auto_fixable_count > 0:
            recommendations.append(f"âš¡ {auto_fixable_count} issues can be automatically fixed")
        
        # ç·åˆã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨äº‹é …
        if report.overall_score < 0.5:
            recommendations.append("ğŸ¯ Consider setting up continuous integration with quality gates")
        elif report.overall_score < 0.8:
            recommendations.append("âœ¨ Good progress! Focus on addressing high-priority issues")
        else:
            recommendations.append("ğŸ‰ Excellent code quality! Maintain current standards")
        
        return recommendations


class QualityGuardian:
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
    
    ä¸»è¦æ©Ÿèƒ½:
    1. ç¶™ç¶šçš„ãªå“è³ªç›£è¦–
    2. å“è³ªé–¾å€¤ã®ç®¡ç†
    3. è‡ªå‹•ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ
    4. å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    5. æ”¹å–„ææ¡ˆã®è‡ªå‹•ç”Ÿæˆ
    """
    
    def __init__(self, config: Dict[str, Any]):
        """åˆæœŸåŒ–"""
        self.config = config
        self.analyzer = QualityAnalyzer(config.get('analyzer', {}))
        self.thresholds: Dict[QualityMetric, QualityThreshold] = {}
        self.reports: Dict[str, QualityReport] = {}
        self.monitoring_paths: Set[Path] = set()
        
        # ãƒ­ã‚°è¨­å®š
        self.logger = logging.getLogger(__name__)
        self._setup_logging()
        
        # è¨­å®šå€¤
        self.monitoring_interval = config.get('monitoring_interval', 300)  # 5åˆ†
        self.alert_webhooks = config.get('alert_webhooks', [])
        self.auto_fix_enabled = config.get('auto_fix_enabled', False)
        
        # åˆæœŸåŒ–çŠ¶æ…‹
        self.is_running = False
        self._background_tasks = set()
        self._shutdown_event = asyncio.Event()
        
        # é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡ç”¨ï¼‰
        self.communication_protocol: Optional[CommunicationProtocol] = None
    
    def _setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_dir = Path.home() / '.claude' / 'logs' / 'shunsuke-ecosystem'
        log_dir.mkdir(parents=True, exist_ok=True)
        
        handler = logging.FileHandler(log_dir / 'quality-guardian.log')
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    async def initialize(self):
        """å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            self.logger.info("Quality Guardian initialization started")
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå“è³ªé–¾å€¤è¨­å®š
            await self._setup_default_thresholds()
            
            # é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«åˆæœŸåŒ–
            if self.config.get('enable_communication', True):
                self.communication_protocol = CommunicationProtocol(
                    "quality_guardian",
                    self.config.get('communication', {})
                )
                await self.communication_protocol.initialize()
            
            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ç›£è¦–é–‹å§‹
            await self._start_monitoring()
            
            self.is_running = True
            self.logger.info("Quality Guardian initialization completed")
            
        except Exception as e:
            self.logger.error(f"Quality Guardian initialization failed: {e}")
            raise
    
    async def _setup_default_thresholds(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå“è³ªé–¾å€¤è¨­å®š"""
        default_thresholds = {
            QualityMetric.CODE_COMPLEXITY: QualityThreshold(
                metric=QualityMetric.CODE_COMPLEXITY,
                min_acceptable=0.6,
                target=0.8,
                critical_threshold=0.3
            ),
            QualityMetric.TEST_COVERAGE: QualityThreshold(
                metric=QualityMetric.TEST_COVERAGE,
                min_acceptable=0.7,
                target=0.9,
                critical_threshold=0.5
            ),
            QualityMetric.SECURITY: QualityThreshold(
                metric=QualityMetric.SECURITY,
                min_acceptable=0.9,
                target=1.0,
                critical_threshold=0.7
            ),
            QualityMetric.DOCUMENTATION: QualityThreshold(
                metric=QualityMetric.DOCUMENTATION,
                min_acceptable=0.6,
                target=0.8,
                critical_threshold=0.4
            )
        }
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã¨ãƒãƒ¼ã‚¸
        user_thresholds = self.config.get('thresholds', {})
        for metric_name, threshold_config in user_thresholds.items():
            try:
                metric = QualityMetric(metric_name)
                if metric in default_thresholds:
                    # æ—¢å­˜é–¾å€¤ã‚’æ›´æ–°
                    threshold = default_thresholds[metric]
                    threshold.min_acceptable = threshold_config.get('min_acceptable', threshold.min_acceptable)
                    threshold.target = threshold_config.get('target', threshold.target)
                    threshold.critical_threshold = threshold_config.get('critical_threshold', threshold.critical_threshold)
                    threshold.enabled = threshold_config.get('enabled', threshold.enabled)
            except ValueError:
                self.logger.warning(f"Unknown quality metric in config: {metric_name}")
        
        self.thresholds = default_thresholds
        self.logger.info(f"Configured {len(self.thresholds)} quality thresholds")
    
    async def _start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        # ç¶™ç¶šç›£è¦–ã‚¿ã‚¹ã‚¯
        monitor_task = asyncio.create_task(self._continuous_monitoring())
        self._background_tasks.add(monitor_task)
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆå‡¦ç†ã‚¿ã‚¹ã‚¯
        alert_task = asyncio.create_task(self._alert_processor())
        self._background_tasks.add(alert_task)
    
    async def add_monitoring_path(self, path: Path):
        """ç›£è¦–ãƒ‘ã‚¹è¿½åŠ """
        if path.exists():
            self.monitoring_paths.add(path)
            self.logger.info(f"Added monitoring path: {path}")
        else:
            self.logger.warning(f"Monitoring path does not exist: {path}")
    
    async def remove_monitoring_path(self, path: Path):
        """ç›£è¦–ãƒ‘ã‚¹å‰Šé™¤"""
        self.monitoring_paths.discard(path)
        self.logger.info(f"Removed monitoring path: {path}")
    
    async def analyze_project_quality(self, project_path: Path) -> QualityReport:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå“è³ªåˆ†æ"""
        self.logger.info(f"Starting quality analysis for: {project_path}")
        
        # å“è³ªåˆ†æå®Ÿè¡Œ
        report = await self.analyzer.analyze_project(project_path)
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        self.reports[report.id] = report
        
        # é–¾å€¤ãƒã‚§ãƒƒã‚¯
        violations = await self._check_thresholds(report)
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ
        if violations:
            await self._generate_alerts(report, violations)
        
        # è‡ªå‹•ä¿®æ­£å®Ÿè¡Œï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        if self.auto_fix_enabled and report.auto_fixable_issues:
            await self._auto_fix_issues(project_path, report.auto_fixable_issues)
        
        self.logger.info(f"Quality analysis completed: {report.overall_score:.2f} score, {len(report.issues)} issues")
        
        return report
    
    async def _check_thresholds(self, report: QualityReport) -> List[Dict[str, Any]]:
        """å“è³ªé–¾å€¤ãƒã‚§ãƒƒã‚¯"""
        violations = []
        
        for metric, score in report.metric_scores.items():
            if metric in self.thresholds:
                threshold = self.thresholds[metric]
                
                if not threshold.enabled:
                    continue
                
                if score < threshold.critical_threshold:
                    violations.append({
                        'metric': metric,
                        'severity': 'critical',
                        'score': score,
                        'threshold': threshold.critical_threshold,
                        'message': f"{metric.value} score ({score:.2f}) is below critical threshold ({threshold.critical_threshold:.2f})"
                    })
                elif score < threshold.min_acceptable:
                    violations.append({
                        'metric': metric,
                        'severity': 'warning',
                        'score': score,
                        'threshold': threshold.min_acceptable,
                        'message': f"{metric.value} score ({score:.2f}) is below acceptable threshold ({threshold.min_acceptable:.2f})"
                    })
        
        return violations
    
    async def _generate_alerts(self, report: QualityReport, violations: List[Dict[str, Any]]):
        """ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ"""
        alert_data = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'project_path': report.project_path,
            'report_id': report.id,
            'overall_score': report.overall_score,
            'violations': violations,
            'critical_issues': len(report.critical_issues),
            'total_issues': len(report.issues)
        }
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€šä¿¡ã§ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
        if self.communication_protocol:
            await self.communication_protocol.broadcast_message(
                message_type=MessageType.ERROR,
                payload=alert_data
            )
        
        # Webhooké€šçŸ¥
        for webhook_url in self.alert_webhooks:
            try:
                # å®Ÿéš›ã®å®Ÿè£…ã§ã¯HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
                self.logger.info(f"Alert sent to webhook: {webhook_url}")
            except Exception as e:
                self.logger.error(f"Failed to send alert to webhook {webhook_url}: {e}")
        
        self.logger.warning(f"Quality alerts generated for {report.project_path}: {len(violations)} violations")
    
    async def _auto_fix_issues(self, project_path: Path, issues: List[QualityIssue]):
        """è‡ªå‹•ä¿®æ­£å®Ÿè¡Œ"""
        fixed_count = 0
        
        for issue in issues:
            try:
                if await self._fix_issue(project_path, issue):
                    fixed_count += 1
            except Exception as e:
                self.logger.error(f"Auto-fix failed for issue {issue.id}: {e}")
        
        if fixed_count > 0:
            self.logger.info(f"Auto-fixed {fixed_count}/{len(issues)} issues")
    
    async def _fix_issue(self, project_path: Path, issue: QualityIssue) -> bool:
        """å€‹åˆ¥å•é¡Œã®ä¿®æ­£"""
        if not issue.auto_fixable:
            return False
        
        try:
            if issue.rule_id == "long_line":
                return await self._fix_long_line(issue)
            elif issue.rule_id == "tab_char":
                return await self._fix_tab_characters(issue)
            # ä»–ã®è‡ªå‹•ä¿®æ­£ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ 
            
        except Exception as e:
            self.logger.error(f"Failed to fix issue {issue.id}: {e}")
        
        return False
    
    async def _fix_long_line(self, issue: QualityIssue) -> bool:
        """é•·ã„è¡Œã®ä¿®æ­£"""
        # å®Ÿè£…ä¾‹ï¼šé•·ã„è¡Œã‚’é©åˆ‡ã«æ”¹è¡Œ
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€æ§‹æ–‡è§£æã‚’è€ƒæ…®ã—ãŸæ”¹è¡ŒãŒå¿…è¦
        return False
    
    async def _fix_tab_characters(self, issue: QualityIssue) -> bool:
        """ã‚¿ãƒ–æ–‡å­—ã®ä¿®æ­£"""
        if not issue.file_path:
            return False
        
        try:
            file_path = Path(issue.file_path)
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ã‚¿ãƒ–ã‚’4ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
            fixed_content = content.replace('\t', '    ')
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(fixed_content)
            
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to fix tab characters in {issue.file_path}: {e}")
            return False
    
    async def _continuous_monitoring(self):
        """ç¶™ç¶šçš„ç›£è¦–"""
        while not self._shutdown_event.is_set():
            try:
                # ç›£è¦–å¯¾è±¡ãƒ‘ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
                for path in list(self.monitoring_paths):
                    if path.exists():
                        await self.analyze_project_quality(path)
                    else:
                        self.logger.warning(f"Monitoring path no longer exists: {path}")
                        self.monitoring_paths.discard(path)
                
                # æ¬¡ã®ç›£è¦–ã¾ã§å¾…æ©Ÿ
                await asyncio.sleep(self.monitoring_interval)
                
            except Exception as e:
                self.logger.error(f"Continuous monitoring error: {e}")
                await asyncio.sleep(60)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯1åˆ†å¾Œã«ãƒªãƒˆãƒ©ã‚¤
    
    async def _alert_processor(self):
        """ã‚¢ãƒ©ãƒ¼ãƒˆå‡¦ç†"""
        while not self._shutdown_event.is_set():
            try:
                # æœªå‡¦ç†ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’å‡¦ç†
                # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚¢ãƒ©ãƒ¼ãƒˆã‚­ãƒ¥ãƒ¼ã‹ã‚‰å‡¦ç†
                await asyncio.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Alert processor error: {e}")
    
    async def get_quality_summary(self) -> Dict[str, Any]:
        """å“è³ªã‚µãƒãƒªãƒ¼å–å¾—"""
        if not self.reports:
            return {"message": "No quality reports available"}
        
        latest_reports = sorted(self.reports.values(), key=lambda r: r.timestamp, reverse=True)[:10]
        
        # çµ±è¨ˆè¨ˆç®—
        overall_scores = [r.overall_score for r in latest_reports]
        
        return {
            "total_reports": len(self.reports),
            "latest_reports_count": len(latest_reports),
            "average_score": statistics.mean(overall_scores) if overall_scores else 0,
            "score_trend": "improving" if len(overall_scores) > 1 and overall_scores[0] > overall_scores[-1] else "stable",
            "monitoring_paths": len(self.monitoring_paths),
            "active_thresholds": len([t for t in self.thresholds.values() if t.enabled])
        }
    
    async def get_status(self) -> Dict[str, Any]:
        """å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹å–å¾—"""
        return {
            "is_running": self.is_running,
            "monitoring_paths": len(self.monitoring_paths),
            "quality_reports": len(self.reports),
            "active_thresholds": len([t for t in self.thresholds.values() if t.enabled]),
            "auto_fix_enabled": self.auto_fix_enabled,
            "monitoring_interval": self.monitoring_interval,
            "communication_enabled": self.communication_protocol is not None
        }
    
    async def shutdown(self):
        """å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        self.logger.info("Quality Guardian shutdown initiated")
        
        # ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆè¨­å®š
        self._shutdown_event.set()
        
        # é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
        if self.communication_protocol:
            await self.communication_protocol.shutdown()
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã®çµ‚äº†ã‚’å¾…æ©Ÿ
        if self._background_tasks:
            await asyncio.gather(*self._background_tasks, return_exceptions=True)
        
        self.is_running = False
        self.logger.info("Quality Guardian shutdown completed")


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    async def test_quality_guardian():
        config = {
            'analyzer': {},
            'monitoring_interval': 10,
            'auto_fix_enabled': True,
            'thresholds': {
                'code_complexity': {
                    'min_acceptable': 0.7,
                    'target': 0.9,
                    'critical_threshold': 0.4
                }
            }
        }
        
        guardian = QualityGuardian(config)
        await guardian.initialize()
        
        # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿½åŠ 
        test_project = Path.cwd()
        await guardian.add_monitoring_path(test_project)
        
        # å“è³ªåˆ†æå®Ÿè¡Œ
        report = await guardian.analyze_project_quality(test_project)
        
        print("Quality Report:")
        print(f"Overall Score: {report.overall_score:.2f}")
        print(f"Total Issues: {len(report.issues)}")
        print(f"Critical Issues: {len(report.critical_issues)}")
        print(f"Auto-fixable Issues: {len(report.auto_fixable_issues)}")
        
        # å“è³ªã‚µãƒãƒªãƒ¼å–å¾—
        summary = await guardian.get_quality_summary()
        print("Quality Summary:", json.dumps(summary, indent=2))
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
        status = await guardian.get_status()
        print("Guardian Status:", json.dumps(status, indent=2))
        
        await guardian.shutdown()
    
    asyncio.run(test_quality_guardian())