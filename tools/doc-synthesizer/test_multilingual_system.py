#!/usr/bin/env python3
"""
ã‚·ãƒ¥ãƒ³ã‚¹ã‚±å¼å¤šè¨€èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆ - Ultimate ShunsukeModel Ecosystem

å¤šè¨€èªå¯¾å¿œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import json
import logging
import tempfile
import shutil
from pathlib import Path
from typing import Dict, Any, List
import sys
import os

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .multilingual_manager import (
        MultilingualManager, LanguageCode, DocumentStyle, 
        TranslationRequest, create_multilingual_manager
    )
    from .multilingual_templates import (
        MultilingualTemplateManager, TemplateType,
        create_multilingual_template_manager
    )
    from .documentation_synthesizer import (
        DocumentationSynthesizer, DocumentationConfig,
        DocumentType, DocumentFormat, DocumentationQuality, Language
    )
    from .multilingual_documentation_methods import add_multilingual_methods_to_synthesizer
except ImportError as e:
    print(f"Import error: {e}")
    print("Running tests from parent directory...")
    sys.path.append(str(Path(__file__).parent.parent.parent))
    
    from tools.doc_synthesizer.multilingual_manager import (
        MultilingualManager, LanguageCode, DocumentStyle, 
        TranslationRequest, create_multilingual_manager
    )
    from tools.doc_synthesizer.multilingual_templates import (
        MultilingualTemplateManager, TemplateType,
        create_multilingual_template_manager
    )
    from tools.doc_synthesizer.documentation_synthesizer import (
        DocumentationSynthesizer, DocumentationConfig,
        DocumentType, DocumentFormat, DocumentationQuality, Language
    )
    from tools.doc_synthesizer.multilingual_documentation_methods import add_multilingual_methods_to_synthesizer


class MultilingualSystemTest:
    """å¤šè¨€èªã‚·ã‚¹ãƒ†ãƒ åŒ…æ‹¬ãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.setup_logging()
        
        # ãƒ†ã‚¹ãƒˆç”¨ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.test_dir = None
        self.project_dir = None
        self.output_dir = None
        
        # ãƒ†ã‚¹ãƒˆçµæœ
        self.test_results = {
            'tests_run': 0,
            'tests_passed': 0,
            'tests_failed': 0,
            'errors': []
        }
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¿½åŠ 
        log_dir = Path.home() / '.claude' / 'logs' / 'shunsuke-ecosystem' / 'tests'
        log_dir.mkdir(parents=True, exist_ok=True)
        
        file_handler = logging.FileHandler(log_dir / 'multilingual_test.log')
        file_handler.setFormatter(
            logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        )
        self.logger.addHandler(file_handler)
    
    async def setup_test_environment(self):
        """ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            self.test_dir = Path(tempfile.mkdtemp(prefix="multilingual_test_"))
            self.project_dir = self.test_dir / "test_project"
            self.output_dir = self.test_dir / "output"
            
            # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ä½œæˆ
            await self._create_test_project_structure()
            
            self.logger.info(f"Test environment setup completed: {self.test_dir}")
            
        except Exception as e:
            self.logger.error(f"Test environment setup failed: {e}")
            raise
    
    async def _create_test_project_structure(self):
        """ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ä½œæˆ"""
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.project_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚µãƒ³ãƒ—ãƒ«Pythonãƒ•ã‚¡ã‚¤ãƒ«
        (self.project_dir / "main.py").write_text('''
"""Sample main module for testing"""

class TestClass:
    """A test class for documentation generation"""
    
    def __init__(self, name: str):
        """Initialize with name"""
        self.name = name
    
    def greet(self) -> str:
        """Return a greeting message"""
        return f"Hello, {self.name}!"

def main():
    """Main function"""
    test = TestClass("World")
    print(test.greet())

if __name__ == "__main__":
    main()
''')
        
        # setup.py
        (self.project_dir / "setup.py").write_text('''
from setuptools import setup, find_packages

setup(
    name="test-project",
    version="1.0.0",
    description="A test project for multilingual documentation",
    packages=find_packages(),
    python_requires=">=3.9",
)
''')
        
        # requirements.txt
        (self.project_dir / "requirements.txt").write_text('''
aiohttp>=3.8.0
pydantic>=2.0.0
''')
        
        # README.md
        (self.project_dir / "README.md").write_text('''
# Test Project

This is a test project for multilingual documentation generation.

## Features

- Test functionality
- Documentation generation
- Quality assurance
''')
    
    async def test_multilingual_manager(self) -> Dict[str, Any]:
        """å¤šè¨€èªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        test_name = "MultilingualManager Test"
        self.test_results['tests_run'] += 1
        
        try:
            self.logger.info(f"Starting {test_name}")
            
            # MultilingualManager ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
            manager = await create_multilingual_manager()
            
            # 1. è¨€èªæ¤œå‡ºãƒ†ã‚¹ãƒˆ
            text_en = "This is a test document for language detection."
            lang_en, confidence_en = await manager.detect_language(text_en)
            
            text_ja = "ã“ã‚Œã¯è¨€èªæ¤œå‡ºã®ãŸã‚ã®ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚"
            lang_ja, confidence_ja = await manager.detect_language(text_ja)
            
            assert lang_en == LanguageCode.ENGLISH, f"Expected English, got {lang_en}"
            assert lang_ja == LanguageCode.JAPANESE, f"Expected Japanese, got {lang_ja}"
            
            # 2. ç¿»è¨³ãƒ†ã‚¹ãƒˆ
            translation_request = TranslationRequest(
                source_language=LanguageCode.ENGLISH,
                target_language=LanguageCode.JAPANESE,
                text="Quality Analysis System",
                style=DocumentStyle.TECHNICAL
            )
            
            translation_result = await manager.translate_text(translation_request)
            
            assert translation_result.translated_text, "Translation result should not be empty"
            assert translation_result.source_language == LanguageCode.ENGLISH
            assert translation_result.target_language == LanguageCode.JAPANESE
            
            # 3. ã‚µãƒãƒ¼ãƒˆè¨€èªãƒªã‚¹ãƒˆå–å¾—
            supported_languages = await manager.get_supported_languages()
            assert len(supported_languages) > 0, "Should have supported languages"
            
            # 4. ç¿»è¨³å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
            quality_report = await manager.get_translation_quality_report()
            assert 'total_translations' in quality_report
            
            self.test_results['tests_passed'] += 1
            self.logger.info(f"{test_name} PASSED")
            
            return {
                'status': 'passed',
                'language_detection': {
                    'english': {'detected': lang_en.value, 'confidence': confidence_en},
                    'japanese': {'detected': lang_ja.value, 'confidence': confidence_ja}
                },
                'translation': {
                    'original': translation_request.text,
                    'translated': translation_result.translated_text,
                    'confidence': translation_result.confidence_score
                },
                'supported_languages_count': len(supported_languages)
            }
            
        except Exception as e:
            self.test_results['tests_failed'] += 1
            self.test_results['errors'].append(f"{test_name}: {str(e)}")
            self.logger.error(f"{test_name} FAILED: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    async def test_multilingual_templates(self) -> Dict[str, Any]:
        """å¤šè¨€èªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        test_name = "MultilingualTemplates Test"
        self.test_results['tests_run'] += 1
        
        try:
            self.logger.info(f"Starting {test_name}")
            
            # MultilingualTemplateManager ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
            manager = await create_multilingual_template_manager()
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
            project_data = {
                'name': 'Test Project',
                'description': 'A test project for multilingual templates',
                'python_version': '3.9',
                'author': 'Test Author',
                'year': '2025',
                'features': ['Feature 1', 'Feature 2', 'Feature 3']
            }
            
            # 1. æ—¥æœ¬èªREADMEç”Ÿæˆ
            ja_readme = await manager.generate_document(
                TemplateType.README,
                LanguageCode.JAPANESE,
                project_data,
                DocumentStyle.TECHNICAL
            )
            
            assert ja_readme, "Japanese README should not be empty"
            assert 'Test Project' in ja_readme
            assert 'æ¦‚è¦' in ja_readme or 'ç›®æ¬¡' in ja_readme
            
            # 2. è‹±èªREADMEç”Ÿæˆ
            en_readme = await manager.generate_document(
                TemplateType.README,
                LanguageCode.ENGLISH,
                project_data,
                DocumentStyle.TECHNICAL
            )
            
            assert en_readme, "English README should not be empty"
            assert 'Test Project' in en_readme
            assert 'Table of Contents' in en_readme or 'Overview' in en_readme
            
            # 3. å¤šè¨€èªä¸€æ‹¬ç”Ÿæˆ
            multilingual_docs = await manager.generate_multilingual_docs(
                TemplateType.README,
                project_data,
                [LanguageCode.JAPANESE, LanguageCode.ENGLISH],
                DocumentStyle.TECHNICAL
            )
            
            assert len(multilingual_docs) == 2, "Should generate 2 language versions"
            assert 'ja' in multilingual_docs
            assert 'en' in multilingual_docs
            
            # 4. ã‚µãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¿ã‚¤ãƒ—å–å¾—
            template_types = await manager.get_supported_template_types()
            assert len(template_types) > 0, "Should have supported template types"
            
            self.test_results['tests_passed'] += 1
            self.logger.info(f"{test_name} PASSED")
            
            return {
                'status': 'passed',
                'japanese_readme_length': len(ja_readme),
                'english_readme_length': len(en_readme),
                'multilingual_docs_count': len(multilingual_docs),
                'supported_template_types': len(template_types)
            }
            
        except Exception as e:
            self.test_results['tests_failed'] += 1
            self.test_results['errors'].append(f"{test_name}: {str(e)}")
            self.logger.error(f"{test_name} FAILED: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    async def test_integrated_documentation_synthesizer(self) -> Dict[str, Any]:
        """çµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        test_name = "Integrated DocumentationSynthesizer Test"
        self.test_results['tests_run'] += 1
        
        try:
            self.logger.info(f"Starting {test_name}")
            
            # å¤šè¨€èªãƒ¡ã‚½ãƒƒãƒ‰ã‚’çµ±åˆ
            add_multilingual_methods_to_synthesizer()
            
            # DocumentationSynthesizer è¨­å®š
            config = DocumentationConfig(
                project_path=self.project_dir,
                output_path=self.output_dir,
                languages=[Language.ENGLISH, Language.JAPANESE],
                formats=[DocumentFormat.MARKDOWN],
                quality_level=DocumentationQuality.STANDARD
            )
            
            # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            synthesizer = DocumentationSynthesizer(config)
            await synthesizer.initialize()
            
            # 1. å®Œå…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
            result = await synthesizer.synthesize_complete_documentation([
                DocumentType.README,
                DocumentType.API_REFERENCE
            ])
            
            assert result['status'] == 'completed', f"Generation failed: {result.get('error', 'Unknown error')}"
            assert len(result['generated_files']) > 0, "Should generate files"
            
            # 2. å¤šè¨€èªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼ç”Ÿæˆãƒ†ã‚¹ãƒˆ
            if hasattr(synthesizer, 'generate_multilingual_project_summary'):
                summary_result = await synthesizer.generate_multilingual_project_summary()
                assert summary_result['status'] == 'completed'
                assert len(summary_result['summaries']) > 0
            
            # 3. ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
            generated_files = result['generated_files']
            files_exist = all(Path(file_path).exists() for file_path in generated_files)
            assert files_exist, "All generated files should exist"
            
            # 4. çµ±è¨ˆæƒ…å ±ç¢ºèª
            stats = await synthesizer.get_generation_statistics()
            assert 'current_stats' in stats
            assert stats['current_stats']['documents_generated'] > 0
            
            self.test_results['tests_passed'] += 1
            self.logger.info(f"{test_name} PASSED")
            
            return {
                'status': 'passed',
                'generated_files_count': len(generated_files),
                'documents_generated': result['generation_stats']['documents_generated'],
                'languages_processed': result['generation_stats']['languages_processed'],
                'execution_time': result['execution_time'],
                'files_exist': files_exist
            }
            
        except Exception as e:
            self.test_results['tests_failed'] += 1
            self.test_results['errors'].append(f"{test_name}: {str(e)}")
            self.logger.error(f"{test_name} FAILED: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    async def test_quality_assurance(self) -> Dict[str, Any]:
        """å“è³ªä¿è¨¼æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        test_name = "Quality Assurance Test"
        self.test_results['tests_run'] += 1
        
        try:
            self.logger.info(f"Starting {test_name}")
            
            # ãƒ†ã‚¹ãƒˆç”¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            test_doc_path = self.output_dir / "test_quality.md"
            test_doc_content = '''# Test Document

This is a test document for quality analysis.

## Features

- Feature 1
- Feature 2

## Code Example

```python
def hello():
    return "Hello, World!"
```

## Links

[GitHub](https://github.com/test/repo)
'''
            
            test_doc_path.write_text(test_doc_content)
            
            # DocumentationSynthesizer ã§å“è³ªãƒã‚§ãƒƒã‚¯
            config = DocumentationConfig(
                project_path=self.project_dir,
                output_path=self.output_dir,
                languages=[Language.ENGLISH],
                formats=[DocumentFormat.MARKDOWN],
                quality_level=DocumentationQuality.HIGH
            )
            
            synthesizer = DocumentationSynthesizer(config)
            await synthesizer.initialize()
            
            # å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
            quality_report = await synthesizer._perform_quality_check([str(test_doc_path)])
            
            assert 'file_sizes' in quality_report
            assert 'content_analysis' in quality_report
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æçµæœç¢ºèª
            analysis = quality_report['content_analysis'][str(test_doc_path)]
            assert analysis['has_code_examples'] == True
            assert analysis['has_links'] == True
            assert analysis['header_count'] >= 2
            
            self.test_results['tests_passed'] += 1
            self.logger.info(f"{test_name} PASSED")
            
            return {
                'status': 'passed',
                'quality_report': quality_report,
                'has_code_examples': analysis['has_code_examples'],
                'has_links': analysis['has_links'],
                'header_count': analysis['header_count']
            }
            
        except Exception as e:
            self.test_results['tests_failed'] += 1
            self.test_results['errors'].append(f"{test_name}: {str(e)}")
            self.logger.error(f"{test_name} FAILED: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    async def cleanup_test_environment(self):
        """ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if self.test_dir and self.test_dir.exists():
                shutil.rmtree(self.test_dir)
                self.logger.info(f"Test environment cleaned up: {self.test_dir}")
        except Exception as e:
            self.logger.error(f"Cleanup failed: {e}")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        self.logger.info("Starting comprehensive multilingual system tests")
        
        test_results = {}
        
        try:
            # ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            await self.setup_test_environment()
            
            # å€‹åˆ¥ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_results['multilingual_manager'] = await self.test_multilingual_manager()
            test_results['multilingual_templates'] = await self.test_multilingual_templates()
            test_results['integrated_synthesizer'] = await self.test_integrated_documentation_synthesizer()
            test_results['quality_assurance'] = await self.test_quality_assurance()
            
        except Exception as e:
            self.logger.error(f"Test execution failed: {e}")
            test_results['setup_error'] = str(e)
        
        finally:
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            await self.cleanup_test_environment()
        
        # ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼
        test_summary = {
            'overall_status': 'passed' if self.test_results['tests_failed'] == 0 else 'failed',
            'tests_run': self.test_results['tests_run'],
            'tests_passed': self.test_results['tests_passed'],
            'tests_failed': self.test_results['tests_failed'],
            'success_rate': f"{(self.test_results['tests_passed'] / max(self.test_results['tests_run'], 1)) * 100:.1f}%",
            'errors': self.test_results['errors'],
            'detailed_results': test_results
        }
        
        self.logger.info(f"All tests completed: {test_summary['overall_status']}")
        self.logger.info(f"Success rate: {test_summary['success_rate']}")
        
        return test_summary


async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Ultimate ShunsukeModel Ecosystem - Multilingual Documentation System Test")
    print("=" * 80)
    
    # ãƒ†ã‚¹ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    test_runner = MultilingualSystemTest()
    
    # å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    results = await test_runner.run_all_tests()
    
    # çµæœè¡¨ç¤º
    print("\nğŸ“Š Test Results Summary:")
    print(f"Overall Status: {'âœ… PASSED' if results['overall_status'] == 'passed' else 'âŒ FAILED'}")
    print(f"Tests Run: {results['tests_run']}")
    print(f"Tests Passed: {results['tests_passed']}")
    print(f"Tests Failed: {results['tests_failed']}")
    print(f"Success Rate: {results['success_rate']}")
    
    if results['errors']:
        print("\nâŒ Errors:")
        for error in results['errors']:
            print(f"  - {error}")
    
    print("\nğŸ” Detailed Results:")
    for test_name, test_result in results['detailed_results'].items():
        status = "âœ… PASSED" if test_result.get('status') == 'passed' else "âŒ FAILED"
        print(f"  {test_name}: {status}")
        if test_result.get('error'):
            print(f"    Error: {test_result['error']}")
    
    # JSONçµæœå‡ºåŠ›
    output_file = Path.home() / '.claude' / 'logs' / 'shunsuke-ecosystem' / 'multilingual_test_results.json'
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ“„ Detailed results saved to: {output_file}")
    print("\nğŸ‰ Multilingual Documentation System Test Completed!")
    
    return results


if __name__ == "__main__":
    # asyncioå®Ÿè¡Œ
    results = asyncio.run(main())
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰è¨­å®š
    exit_code = 0 if results['overall_status'] == 'passed' else 1
    sys.exit(exit_code)