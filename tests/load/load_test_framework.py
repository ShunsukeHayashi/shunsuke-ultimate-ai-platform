#!/usr/bin/env python3
"""
ã‚·ãƒ¥ãƒ³ã‚¹ã‚±å¼è² è·ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - Ultimate ShunsukeModel Ecosystem

ã‚·ã‚¹ãƒ†ãƒ ã®è² è·è€æ€§ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é™ç•Œã‚’
åŒ…æ‹¬çš„ã«æ¤œè¨¼ã™ã‚‹é«˜ç²¾åº¦è² è·ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import aiohttp
import time
import logging
import json
import random
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Callable, Union, Tuple
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict, deque
import numpy as np
import statistics
from enum import Enum
import psutil
import sys
import os
from concurrent.futures import ThreadPoolExecutor
import multiprocessing
import signal
import yaml


# ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(str(Path(__file__).parent.parent.parent))

from tools.performance_suite import create_resource_monitor
from tools.performance_suite.resource_monitor import ResourceMonitor, Alert, AlertLevel


class LoadPattern(Enum):
    """è² è·ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    CONSTANT = "constant"           # ä¸€å®šè² è·
    RAMP_UP = "ramp_up"            # å¾ã€…ã«å¢—åŠ 
    SPIKE = "spike"                # ã‚¹ãƒ‘ã‚¤ã‚¯è² è·
    WAVE = "wave"                  # æ³¢çŠ¶è² è·
    STRESS = "stress"              # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
    BREAKPOINT = "breakpoint"      # é™ç•Œç‚¹æ¢ç´¢
    CUSTOM = "custom"              # ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³


class RequestType(Enum):
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ—"""
    GET = "GET"
    POST = "POST"
    PUT = "PUT"
    DELETE = "DELETE"
    COMMAND = "COMMAND"
    QUERY = "QUERY"
    STREAM = "STREAM"


@dataclass
class LoadTestConfig:
    """è² è·ãƒ†ã‚¹ãƒˆè¨­å®š"""
    name: str
    description: str
    target_url: Optional[str] = None
    target_function: Optional[Callable] = None
    load_pattern: LoadPattern = LoadPattern.CONSTANT
    duration: float = 60.0  # ç§’
    users: int = 10
    ramp_up_time: float = 10.0
    requests_per_user: int = 100
    think_time: float = 1.0  # ãƒ¦ãƒ¼ã‚¶ãƒ¼é–“ã®å¾…æ©Ÿæ™‚é–“
    timeout: float = 30.0
    success_criteria: Dict[str, Any] = field(default_factory=dict)
    custom_headers: Dict[str, str] = field(default_factory=dict)
    custom_payload: Optional[Any] = None
    tags: List[str] = field(default_factory=list)


@dataclass
class RequestResult:
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆçµæœ"""
    request_id: str
    user_id: int
    request_type: RequestType
    start_time: float
    end_time: float
    duration: float
    status_code: Optional[int] = None
    success: bool = False
    error: Optional[str] = None
    response_size: int = 0
    custom_metrics: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LoadTestResult:
    """è² è·ãƒ†ã‚¹ãƒˆçµæœ"""
    config: LoadTestConfig
    start_time: datetime
    end_time: datetime
    duration: float
    total_requests: int
    successful_requests: int
    failed_requests: int
    error_rate: float
    requests_per_second: float
    average_response_time: float
    min_response_time: float
    max_response_time: float
    percentiles: Dict[str, float]  # 50th, 90th, 95th, 99th
    response_time_distribution: List[float]
    error_distribution: Dict[str, int]
    throughput: float  # bytes/sec
    concurrent_users: List[int]
    resource_usage: Dict[str, Any]
    bottlenecks: List[Dict[str, Any]]
    recommendations: List[str]


class VirtualUser:
    """ä»®æƒ³ãƒ¦ãƒ¼ã‚¶ãƒ¼"""
    
    def __init__(self, user_id: int, config: LoadTestConfig, session: aiohttp.ClientSession):
        self.user_id = user_id
        self.config = config
        self.session = session
        self.request_count = 0
        self.results: List[RequestResult] = []
        self.active = True
        
    async def run(self) -> List[RequestResult]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        for i in range(self.config.requests_per_user):
            if not self.active:
                break
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            result = await self._execute_request()
            self.results.append(result)
            self.request_count += 1
            
            # Think time
            if self.config.think_time > 0:
                await asyncio.sleep(self.config.think_time + random.uniform(-0.5, 0.5))
        
        return self.results
    
    async def _execute_request(self) -> RequestResult:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        request_id = f"user_{self.user_id}_req_{self.request_count}"
        start_time = time.time()
        
        result = RequestResult(
            request_id=request_id,
            user_id=self.user_id,
            request_type=RequestType.GET,
            start_time=start_time,
            end_time=start_time,
            duration=0.0
        )
        
        try:
            if self.config.target_url:
                # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                async with self.session.get(
                    self.config.target_url,
                    headers=self.config.custom_headers,
                    timeout=aiohttp.ClientTimeout(total=self.config.timeout)
                ) as response:
                    result.status_code = response.status
                    result.success = 200 <= response.status < 300
                    content = await response.read()
                    result.response_size = len(content)
                    
            elif self.config.target_function:
                # é–¢æ•°å‘¼ã³å‡ºã—
                if asyncio.iscoroutinefunction(self.config.target_function):
                    response = await self.config.target_function(self.config.custom_payload)
                else:
                    response = await asyncio.get_event_loop().run_in_executor(
                        None, self.config.target_function, self.config.custom_payload
                    )
                result.success = True
                result.custom_metrics['response'] = response
                
        except asyncio.TimeoutError:
            result.success = False
            result.error = "Timeout"
        except Exception as e:
            result.success = False
            result.error = str(e)
        finally:
            result.end_time = time.time()
            result.duration = result.end_time - result.start_time
        
        return result
    
    def stop(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼åœæ­¢"""
        self.active = False


class LoadTestFramework:
    """
    ã‚·ãƒ¥ãƒ³ã‚¹ã‚±å¼è² è·ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
    
    æ©Ÿèƒ½:
    - å¤šæ§˜ãªè² è·ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
    - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è‡ªå‹•æ¤œå‡º
    - ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æ
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨å¥¨äº‹é …ç”Ÿæˆ
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        
        # çµæœä¿å­˜å…ˆ
        self.output_dir = Path(self.config.get('output_dir', './load_test_results'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä»®æƒ³ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†
        self.virtual_users: List[VirtualUser] = []
        self.active_users = 0
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
        self.results: List[RequestResult] = []
        self.metrics_history = deque(maxlen=1000)
        
        # ãƒªã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒ¼
        self.resource_monitor: Optional[ResourceMonitor] = None
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'start_time': None,
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_bytes': 0,
            'error_types': defaultdict(int)
        }
        
        # å®Ÿè¡Œåˆ¶å¾¡
        self.is_running = False
        self.stop_signal = False
    
    async def run_load_test(self, config: LoadTestConfig) -> LoadTestResult:
        """è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        self.logger.info(f"Starting load test: {config.name}")
        
        # åˆæœŸåŒ–
        self.is_running = True
        self.stop_signal = False
        self.stats['start_time'] = datetime.now()
        start_time = time.time()
        
        # ãƒªã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒ¼é–‹å§‹
        self.resource_monitor = create_resource_monitor({
            'interval': 1.0,
            'thresholds': {
                'cpu': {'warning': 80, 'critical': 95},
                'memory': {'warning': 85, 'critical': 95}
            }
        })
        await self.resource_monitor.start_monitoring()
        
        try:
            # HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            connector = aiohttp.TCPConnector(limit=config.users * 2)
            async with aiohttp.ClientSession(connector=connector) as session:
                # è² è·ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¿œã˜ãŸå®Ÿè¡Œ
                if config.load_pattern == LoadPattern.CONSTANT:
                    result = await self._run_constant_load(config, session)
                elif config.load_pattern == LoadPattern.RAMP_UP:
                    result = await self._run_ramp_up_load(config, session)
                elif config.load_pattern == LoadPattern.SPIKE:
                    result = await self._run_spike_load(config, session)
                elif config.load_pattern == LoadPattern.WAVE:
                    result = await self._run_wave_load(config, session)
                elif config.load_pattern == LoadPattern.STRESS:
                    result = await self._run_stress_test(config, session)
                elif config.load_pattern == LoadPattern.BREAKPOINT:
                    result = await self._run_breakpoint_test(config, session)
                else:
                    result = await self._run_custom_load(config, session)
            
            # çµæœåˆ†æ
            end_time = time.time()
            duration = end_time - start_time
            
            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³å–å¾—
            resource_report = await self.resource_monitor.get_resource_report()
            
            # æœ€çµ‚çµæœç”Ÿæˆ
            final_result = self._analyze_results(config, duration, resource_report)
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            await self._generate_report(final_result)
            
            return final_result
            
        finally:
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.resource_monitor:
                await self.resource_monitor.stop_monitoring()
            self.is_running = False
    
    async def _run_constant_load(self, config: LoadTestConfig, session: aiohttp.ClientSession) -> LoadTestResult:
        """ä¸€å®šè² è·ãƒ†ã‚¹ãƒˆ"""
        self.logger.info(f"Running constant load test with {config.users} users")
        
        # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼åŒæ™‚èµ·å‹•
        tasks = []
        for i in range(config.users):
            user = VirtualUser(i, config, session)
            self.virtual_users.append(user)
            tasks.append(asyncio.create_task(user.run()))
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãå®Ÿè¡Œ
        try:
            all_results = await asyncio.wait_for(
                asyncio.gather(*tasks),
                timeout=config.duration
            )
            
            # çµæœåé›†
            for user_results in all_results:
                self.results.extend(user_results)
                
        except asyncio.TimeoutError:
            self.logger.info("Test duration reached, stopping users...")
            for user in self.virtual_users:
                user.stop()
    
    async def _run_ramp_up_load(self, config: LoadTestConfig, session: aiohttp.ClientSession) -> LoadTestResult:
        """æ®µéšçš„è² è·å¢—åŠ ãƒ†ã‚¹ãƒˆ"""
        self.logger.info(f"Running ramp-up load test: 0 to {config.users} users over {config.ramp_up_time}s")
        
        users_per_second = config.users / config.ramp_up_time
        tasks = []
        start_time = time.time()
        
        for i in range(config.users):
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼èµ·å‹•ã‚¿ã‚¤ãƒŸãƒ³ã‚°è¨ˆç®—
            delay = i / users_per_second
            
            async def start_user_delayed(user_id: int, delay: float):
                await asyncio.sleep(delay)
                user = VirtualUser(user_id, config, session)
                self.virtual_users.append(user)
                return await user.run()
            
            tasks.append(asyncio.create_task(start_user_delayed(i, delay)))
        
        # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼å®Œäº†å¾…æ©Ÿ
        try:
            all_results = await asyncio.wait_for(
                asyncio.gather(*tasks),
                timeout=config.duration
            )
            
            for user_results in all_results:
                if user_results:  # Noneãƒã‚§ãƒƒã‚¯
                    self.results.extend(user_results)
                    
        except asyncio.TimeoutError:
            self.logger.info("Test duration reached")
            for user in self.virtual_users:
                user.stop()
    
    async def _run_spike_load(self, config: LoadTestConfig, session: aiohttp.ClientSession) -> LoadTestResult:
        """ã‚¹ãƒ‘ã‚¤ã‚¯è² è·ãƒ†ã‚¹ãƒˆ"""
        self.logger.info(f"Running spike load test")
        
        # åŸºæœ¬è² è·
        base_users = max(1, config.users // 4)
        spike_users = config.users
        
        tasks = []
        
        # ãƒ•ã‚§ãƒ¼ã‚º1: ä½è² è·
        self.logger.info(f"Phase 1: Low load ({base_users} users)")
        for i in range(base_users):
            user = VirtualUser(i, config, session)
            self.virtual_users.append(user)
            tasks.append(asyncio.create_task(user.run()))
        
        await asyncio.sleep(config.duration * 0.3)
        
        # ãƒ•ã‚§ãƒ¼ã‚º2: ã‚¹ãƒ‘ã‚¤ã‚¯
        self.logger.info(f"Phase 2: Spike ({spike_users} users)")
        for i in range(base_users, spike_users):
            user = VirtualUser(i, config, session)
            self.virtual_users.append(user)
            tasks.append(asyncio.create_task(user.run()))
        
        await asyncio.sleep(config.duration * 0.4)
        
        # ãƒ•ã‚§ãƒ¼ã‚º3: ä½è² è·ã«æˆ»ã‚‹
        self.logger.info(f"Phase 3: Return to low load")
        for i in range(base_users, spike_users):
            if i < len(self.virtual_users):
                self.virtual_users[i].stop()
        
        await asyncio.sleep(config.duration * 0.3)
        
        # çµæœåé›†
        all_results = await asyncio.gather(*tasks, return_exceptions=True)
        for user_results in all_results:
            if isinstance(user_results, list):
                self.results.extend(user_results)
    
    async def _run_wave_load(self, config: LoadTestConfig, session: aiohttp.ClientSession) -> LoadTestResult:
        """æ³¢çŠ¶è² è·ãƒ†ã‚¹ãƒˆ"""
        self.logger.info(f"Running wave load test")
        
        wave_period = config.duration / 3  # 3æ³¢
        max_users = config.users
        min_users = max(1, config.users // 4)
        
        all_tasks = []
        elapsed_time = 0
        
        while elapsed_time < config.duration:
            # æ³¢ã®ç¾åœ¨ä½ç½®è¨ˆç®—
            wave_position = (elapsed_time % wave_period) / wave_period
            current_users = int(min_users + (max_users - min_users) * abs(np.sin(wave_position * np.pi)))
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°èª¿æ•´
            active_count = len([u for u in self.virtual_users if u.active])
            
            if current_users > active_count:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¿½åŠ 
                for i in range(active_count, current_users):
                    user = VirtualUser(len(self.virtual_users), config, session)
                    self.virtual_users.append(user)
                    all_tasks.append(asyncio.create_task(user.run()))
            elif current_users < active_count:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼å‰Šæ¸›
                for i in range(current_users, active_count):
                    if i < len(self.virtual_users):
                        self.virtual_users[i].stop()
            
            await asyncio.sleep(1)
            elapsed_time += 1
        
        # å…¨ã‚¿ã‚¹ã‚¯å®Œäº†å¾…æ©Ÿ
        if all_tasks:
            all_results = await asyncio.gather(*all_tasks, return_exceptions=True)
            for user_results in all_results:
                if isinstance(user_results, list):
                    self.results.extend(user_results)
    
    async def _run_stress_test(self, config: LoadTestConfig, session: aiohttp.ClientSession) -> LoadTestResult:
        """ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆé™ç•Œã¾ã§è² è·å¢—åŠ ï¼‰"""
        self.logger.info(f"Running stress test - finding breaking point")
        
        initial_users = 10
        user_increment = 10
        current_users = 0
        error_threshold = 0.1  # 10%ã‚¨ãƒ©ãƒ¼ç‡ã§åœæ­¢
        
        all_tasks = []
        phase = 0
        
        while not self.stop_signal:
            phase += 1
            current_users += user_increment
            
            self.logger.info(f"Stress test phase {phase}: {current_users} users")
            
            # æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼è¿½åŠ 
            new_tasks = []
            for i in range(len(self.virtual_users), current_users):
                user = VirtualUser(i, config, session)
                self.virtual_users.append(user)
                task = asyncio.create_task(user.run())
                all_tasks.append(task)
                new_tasks.append(task)
            
            # ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œ
            await asyncio.sleep(30)  # å„ãƒ•ã‚§ãƒ¼ã‚º30ç§’
            
            # ã‚¨ãƒ©ãƒ¼ç‡ãƒã‚§ãƒƒã‚¯
            recent_results = [r for r in self.results if time.time() - r.start_time < 30]
            if recent_results:
                error_rate = sum(1 for r in recent_results if not r.success) / len(recent_results)
                
                if error_rate > error_threshold:
                    self.logger.warning(f"Error threshold reached: {error_rate:.2%}")
                    self.stop_signal = True
                    break
            
            # ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if self.resource_monitor:
                current_metrics = await self.resource_monitor.get_current_metrics()
                cpu_usage = current_metrics.get('cpu', {}).get('usage_percent', 0)
                memory_usage = current_metrics.get('memory', {}).get('usage_percent', 0)
                
                if cpu_usage > 95 or memory_usage > 95:
                    self.logger.warning(f"Resource limit reached - CPU: {cpu_usage}%, Memory: {memory_usage}%")
                    self.stop_signal = True
                    break
            
            # æœ€å¤§ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ãƒã‚§ãƒƒã‚¯
            if current_users >= config.users:
                self.logger.info(f"Maximum users reached: {current_users}")
                break
        
        # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼åœæ­¢
        for user in self.virtual_users:
            user.stop()
        
        # çµæœåé›†
        if all_tasks:
            all_results = await asyncio.gather(*all_tasks, return_exceptions=True)
            for user_results in all_results:
                if isinstance(user_results, list):
                    self.results.extend(user_results)
    
    async def _run_breakpoint_test(self, config: LoadTestConfig, session: aiohttp.ClientSession) -> LoadTestResult:
        """ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ†ã‚¹ãƒˆï¼ˆã‚·ã‚¹ãƒ†ãƒ é™ç•Œç‚¹æ¢ç´¢ï¼‰"""
        self.logger.info(f"Running breakpoint test - finding system limits")
        
        # äºŒåˆ†æ¢ç´¢ã§é™ç•Œç‚¹ã‚’è¦‹ã¤ã‘ã‚‹
        min_users = 1
        max_users = config.users
        breakpoint_users = 0
        tolerance = 5  # ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã®è¨±å®¹èª¤å·®
        
        while max_users - min_users > tolerance:
            current_users = (min_users + max_users) // 2
            self.logger.info(f"Testing with {current_users} users")
            
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            self.virtual_users = []
            self.results = []
            
            tasks = []
            for i in range(current_users):
                user = VirtualUser(i, config, session)
                self.virtual_users.append(user)
                tasks.append(asyncio.create_task(user.run()))
            
            # 1åˆ†é–“å®Ÿè¡Œ
            await asyncio.sleep(60)
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼åœæ­¢
            for user in self.virtual_users:
                user.stop()
            
            # çµæœåé›†
            all_results = await asyncio.gather(*tasks, return_exceptions=True)
            test_results = []
            for user_results in all_results:
                if isinstance(user_results, list):
                    test_results.extend(user_results)
            
            # æˆåŠŸç‡è¨ˆç®—
            if test_results:
                success_rate = sum(1 for r in test_results if r.success) / len(test_results)
                avg_response_time = statistics.mean(r.duration for r in test_results if r.success)
                
                # æˆåŠŸåŸºæº–ãƒã‚§ãƒƒã‚¯
                if success_rate >= 0.95 and avg_response_time < 2.0:  # 95%æˆåŠŸç‡ã€2ç§’ä»¥å†…
                    min_users = current_users
                    breakpoint_users = current_users
                else:
                    max_users = current_users
            else:
                max_users = current_users
        
        self.logger.info(f"Breakpoint found at approximately {breakpoint_users} users")
        
        # æœ€çµ‚çš„ãªçµæœã¨ã—ã¦ breakpoint ã§ã®çµæœã‚’è¿”ã™
        self.results = test_results
    
    async def _run_custom_load(self, config: LoadTestConfig, session: aiohttp.ClientSession) -> LoadTestResult:
        """ã‚«ã‚¹ã‚¿ãƒ è² è·ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        self.logger.info(f"Running custom load pattern")
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…
        # ã“ã“ã§ã¯ç°¡å˜ãªä¾‹ã¨ã—ã¦ã€è¨­å®šã«åŸºã¥ã„ãŸå®Ÿè¡Œã‚’è¡Œã†
        tasks = []
        for i in range(config.users):
            user = VirtualUser(i, config, session)
            self.virtual_users.append(user)
            tasks.append(asyncio.create_task(user.run()))
        
        all_results = await asyncio.gather(*tasks)
        for user_results in all_results:
            self.results.extend(user_results)
    
    def _analyze_results(self, config: LoadTestConfig, duration: float, resource_report: Dict[str, Any]) -> LoadTestResult:
        """çµæœåˆ†æ"""
        if not self.results:
            # çµæœãŒãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            return LoadTestResult(
                config=config,
                start_time=self.stats['start_time'],
                end_time=datetime.now(),
                duration=duration,
                total_requests=0,
                successful_requests=0,
                failed_requests=0,
                error_rate=0.0,
                requests_per_second=0.0,
                average_response_time=0.0,
                min_response_time=0.0,
                max_response_time=0.0,
                percentiles={},
                response_time_distribution=[],
                error_distribution={},
                throughput=0.0,
                concurrent_users=[],
                resource_usage=resource_report,
                bottlenecks=[],
                recommendations=["No test data available"]
            )
        
        # åŸºæœ¬çµ±è¨ˆ
        total_requests = len(self.results)
        successful_requests = sum(1 for r in self.results if r.success)
        failed_requests = total_requests - successful_requests
        error_rate = failed_requests / total_requests if total_requests > 0 else 0
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ çµ±è¨ˆ
        response_times = [r.duration for r in self.results if r.success]
        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            
            # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—
            sorted_times = sorted(response_times)
            percentiles = {
                '50th': np.percentile(sorted_times, 50),
                '90th': np.percentile(sorted_times, 90),
                '95th': np.percentile(sorted_times, 95),
                '99th': np.percentile(sorted_times, 99)
            }
        else:
            avg_response_time = 0
            min_response_time = 0
            max_response_time = 0
            percentiles = {'50th': 0, '90th': 0, '95th': 0, '99th': 0}
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
        total_bytes = sum(r.response_size for r in self.results)
        throughput = total_bytes / duration if duration > 0 else 0
        
        # RPSè¨ˆç®—
        requests_per_second = total_requests / duration if duration > 0 else 0
        
        # ã‚¨ãƒ©ãƒ¼åˆ†å¸ƒ
        error_distribution = defaultdict(int)
        for r in self.results:
            if not r.success and r.error:
                error_distribution[r.error] += 1
        
        # ä¸¦è¡Œãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã®æ™‚ç³»åˆ—
        time_buckets = defaultdict(set)
        for r in self.results:
            bucket = int(r.start_time - self.results[0].start_time)
            time_buckets[bucket].add(r.user_id)
        
        concurrent_users = [len(users) for _, users in sorted(time_buckets.items())]
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º
        bottlenecks = self._detect_bottlenecks(
            response_times, 
            error_rate, 
            resource_report
        )
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendations = self._generate_recommendations(
            avg_response_time,
            error_rate,
            percentiles,
            bottlenecks,
            resource_report
        )
        
        return LoadTestResult(
            config=config,
            start_time=self.stats['start_time'],
            end_time=datetime.now(),
            duration=duration,
            total_requests=total_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            error_rate=error_rate,
            requests_per_second=requests_per_second,
            average_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            percentiles=percentiles,
            response_time_distribution=response_times,
            error_distribution=dict(error_distribution),
            throughput=throughput,
            concurrent_users=concurrent_users,
            resource_usage=resource_report,
            bottlenecks=bottlenecks,
            recommendations=recommendations
        )
    
    def _detect_bottlenecks(self, 
                           response_times: List[float], 
                           error_rate: float,
                           resource_report: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º"""
        bottlenecks = []
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ã®æ€¥æ¿€ãªå¢—åŠ æ¤œå‡º
        if len(response_times) > 10:
            # ç§»å‹•å¹³å‡ã§å‚¾å‘åˆ†æ
            window_size = min(10, len(response_times) // 10)
            moving_avg = []
            
            for i in range(window_size, len(response_times)):
                window = response_times[i-window_size:i]
                moving_avg.append(statistics.mean(window))
            
            if moving_avg:
                # æ€¥æ¿€ãªå¢—åŠ ã‚’æ¤œå‡º
                for i in range(1, len(moving_avg)):
                    if moving_avg[i] > moving_avg[i-1] * 1.5:  # 50%ä»¥ä¸Šã®å¢—åŠ 
                        bottlenecks.append({
                            'type': 'response_time_degradation',
                            'severity': 'high',
                            'description': f'Response time increased by {(moving_avg[i]/moving_avg[i-1] - 1)*100:.1f}% at {i*window_size} requests',
                            'metric': 'response_time',
                            'value': moving_avg[i]
                        })
        
        # ã‚¨ãƒ©ãƒ¼ç‡ãƒ™ãƒ¼ã‚¹ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        if error_rate > 0.05:  # 5%ä»¥ä¸Šã®ã‚¨ãƒ©ãƒ¼
            severity = 'critical' if error_rate > 0.2 else 'high' if error_rate > 0.1 else 'medium'
            bottlenecks.append({
                'type': 'high_error_rate',
                'severity': severity,
                'description': f'Error rate of {error_rate*100:.1f}% exceeds acceptable threshold',
                'metric': 'error_rate',
                'value': error_rate
            })
        
        # ãƒªã‚½ãƒ¼ã‚¹ãƒ™ãƒ¼ã‚¹ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        if resource_report:
            # CPU ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
            cpu_usage = resource_report.get('peak_usage', {}).get('cpu', 0)
            if cpu_usage > 80:
                bottlenecks.append({
                    'type': 'cpu_bottleneck',
                    'severity': 'critical' if cpu_usage > 95 else 'high',
                    'description': f'CPU usage peaked at {cpu_usage:.1f}%',
                    'metric': 'cpu_usage',
                    'value': cpu_usage
                })
            
            # ãƒ¡ãƒ¢ãƒªãƒœãƒˆãƒ«ãƒãƒƒã‚¯
            memory_usage = resource_report.get('peak_usage', {}).get('memory', 0)
            if memory_usage > 85:
                bottlenecks.append({
                    'type': 'memory_bottleneck',
                    'severity': 'critical' if memory_usage > 95 else 'high',
                    'description': f'Memory usage peaked at {memory_usage:.1f}%',
                    'metric': 'memory_usage',
                    'value': memory_usage
                })
        
        return bottlenecks
    
    def _generate_recommendations(self,
                                 avg_response_time: float,
                                 error_rate: float,
                                 percentiles: Dict[str, float],
                                 bottlenecks: List[Dict[str, Any]],
                                 resource_report: Dict[str, Any]) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨äº‹é …
        if avg_response_time > 3.0:
            recommendations.append("âš ï¸ Average response time exceeds 3 seconds. Consider:")
            recommendations.append("  â€¢ Implementing caching mechanisms")
            recommendations.append("  â€¢ Optimizing database queries")
            recommendations.append("  â€¢ Adding more application servers")
        
        if percentiles.get('95th', 0) > avg_response_time * 2:
            recommendations.append("ğŸ“Š High response time variance detected. Consider:")
            recommendations.append("  â€¢ Investigating slow queries or operations")
            recommendations.append("  â€¢ Implementing request queuing")
            recommendations.append("  â€¢ Adding circuit breakers")
        
        # ã‚¨ãƒ©ãƒ¼ç‡ãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨äº‹é …
        if error_rate > 0.01:
            recommendations.append(f"âŒ Error rate of {error_rate*100:.1f}% detected. Consider:")
            recommendations.append("  â€¢ Implementing retry mechanisms")
            recommendations.append("  â€¢ Increasing timeout values")
            recommendations.append("  â€¢ Scaling backend services")
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨äº‹é …
        for bottleneck in bottlenecks:
            if bottleneck['type'] == 'cpu_bottleneck':
                recommendations.append("ğŸ”¥ CPU bottleneck detected. Consider:")
                recommendations.append("  â€¢ Optimizing CPU-intensive operations")
                recommendations.append("  â€¢ Implementing horizontal scaling")
                recommendations.append("  â€¢ Using more efficient algorithms")
                
            elif bottleneck['type'] == 'memory_bottleneck':
                recommendations.append("ğŸ’¾ Memory bottleneck detected. Consider:")
                recommendations.append("  â€¢ Optimizing memory usage")
                recommendations.append("  â€¢ Implementing memory pooling")
                recommendations.append("  â€¢ Increasing available memory")
        
        # ä¸€èˆ¬çš„ãªæ¨å¥¨äº‹é …
        if not recommendations:
            recommendations.append("âœ… System performed well under load")
            recommendations.append("Consider running tests with higher load to find limits")
        
        return recommendations
    
    async def _generate_report(self, result: LoadTestResult):
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # HTMLãƒ¬ãƒãƒ¼ãƒˆ
        html_path = self.output_dir / f"load_test_report_{timestamp}.html"
        html_content = self._generate_html_report(result)
        
        with open(html_path, 'w') as f:
            f.write(html_content)
        
        # JSONãƒ¬ãƒãƒ¼ãƒˆ
        json_path = self.output_dir / f"load_test_results_{timestamp}.json"
        json_data = {
            'config': {
                'name': result.config.name,
                'description': result.config.description,
                'load_pattern': result.config.load_pattern.value,
                'duration': result.config.duration,
                'users': result.config.users,
                'requests_per_user': result.config.requests_per_user
            },
            'summary': {
                'start_time': result.start_time.isoformat(),
                'end_time': result.end_time.isoformat(),
                'duration': result.duration,
                'total_requests': result.total_requests,
                'successful_requests': result.successful_requests,
                'failed_requests': result.failed_requests,
                'error_rate': result.error_rate,
                'requests_per_second': result.requests_per_second,
                'average_response_time': result.average_response_time,
                'throughput_bytes_per_sec': result.throughput
            },
            'response_times': {
                'min': result.min_response_time,
                'max': result.max_response_time,
                'average': result.average_response_time,
                'percentiles': result.percentiles
            },
            'errors': result.error_distribution,
            'bottlenecks': result.bottlenecks,
            'recommendations': result.recommendations,
            'resource_usage': result.resource_usage
        }
        
        with open(json_path, 'w') as f:
            json.dump(json_data, f, indent=2)
        
        self.logger.info(f"Reports generated: {html_path}, {json_path}")
    
    def _generate_html_report(self, result: LoadTestResult) -> str:
        """HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Load Test Report - {result.config.name}</title>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .header {{ background-color: #d32f2f; color: white; padding: 20px; border-radius: 5px; }}
        .summary {{ background-color: white; padding: 20px; margin: 20px 0; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }}
        .metric-card {{ display: inline-block; margin: 10px; padding: 15px; background-color: #fff; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); min-width: 150px; text-align: center; }}
        .metric-value {{ font-size: 24px; font-weight: bold; color: #1976d2; }}
        .metric-label {{ color: #666; margin-top: 5px; }}
        .section {{ background-color: white; padding: 20px; margin: 20px 0; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }}
        .bottleneck {{ padding: 10px; margin: 5px 0; border-radius: 5px; }}
        .bottleneck.critical {{ background-color: #ffebee; border-left: 5px solid #d32f2f; }}
        .bottleneck.high {{ background-color: #fff3e0; border-left: 5px solid #f57c00; }}
        .bottleneck.medium {{ background-color: #fff8e1; border-left: 5px solid #fbc02d; }}
        .recommendation {{ padding: 10px; margin: 5px 0; background-color: #e3f2fd; border-left: 5px solid #1976d2; border-radius: 5px; }}
        .chart {{ margin: 20px 0; }}
        table {{ width: 100%; border-collapse: collapse; }}
        th, td {{ padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #f5f5f5; font-weight: bold; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸš€ Load Test Report: {result.config.name}</h1>
        <p>{result.config.description}</p>
        <p>Test Pattern: {result.config.load_pattern.value} | Duration: {result.duration:.1f}s | Users: {result.config.users}</p>
        <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="summary">
        <h2>ğŸ“Š Test Summary</h2>
        <div style="text-align: center;">
            <div class="metric-card">
                <div class="metric-value">{result.total_requests:,}</div>
                <div class="metric-label">Total Requests</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{result.requests_per_second:.1f}</div>
                <div class="metric-label">Requests/sec</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{result.average_response_time*1000:.0f}ms</div>
                <div class="metric-label">Avg Response Time</div>
            </div>
            <div class="metric-card">
                <div class="metric-value" style="color: {'#d32f2f' if result.error_rate > 0.05 else '#388e3c'}">
                    {result.error_rate*100:.1f}%
                </div>
                <div class="metric-label">Error Rate</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{result.throughput/1024/1024:.1f} MB/s</div>
                <div class="metric-label">Throughput</div>
            </div>
        </div>
    </div>
    
    <div class="section">
        <h2>â±ï¸ Response Time Analysis</h2>
        <table>
            <tr>
                <th>Metric</th>
                <th>Value (ms)</th>
            </tr>
            <tr>
                <td>Minimum</td>
                <td>{result.min_response_time*1000:.1f}</td>
            </tr>
            <tr>
                <td>Average</td>
                <td>{result.average_response_time*1000:.1f}</td>
            </tr>
            <tr>
                <td>50th percentile (Median)</td>
                <td>{result.percentiles.get('50th', 0)*1000:.1f}</td>
            </tr>
            <tr>
                <td>90th percentile</td>
                <td>{result.percentiles.get('90th', 0)*1000:.1f}</td>
            </tr>
            <tr>
                <td>95th percentile</td>
                <td>{result.percentiles.get('95th', 0)*1000:.1f}</td>
            </tr>
            <tr>
                <td>99th percentile</td>
                <td>{result.percentiles.get('99th', 0)*1000:.1f}</td>
            </tr>
            <tr>
                <td>Maximum</td>
                <td>{result.max_response_time*1000:.1f}</td>
            </tr>
        </table>
    </div>
"""
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ åˆ†å¸ƒã‚°ãƒ©ãƒ•
        if result.response_time_distribution:
            html += """
    <div class="section">
        <h2>ğŸ“ˆ Response Time Distribution</h2>
        <div id="responseTimeChart" class="chart"></div>
    </div>
    
    <script>
        var responseData = {
            x: """ + json.dumps([r*1000 for r in result.response_time_distribution]) + """,
            type: 'histogram',
            name: 'Response Time (ms)',
            nbinsx: 50
        };
        
        var layout = {
            title: 'Response Time Distribution',
            xaxis: {title: 'Response Time (ms)'},
            yaxis: {title: 'Frequency'}
        };
        
        Plotly.newPlot('responseTimeChart', [responseData], layout);
    </script>
"""
        
        # ã‚¨ãƒ©ãƒ¼åˆ†å¸ƒ
        if result.error_distribution:
            html += f"""
    <div class="section">
        <h2>âŒ Error Distribution</h2>
        <table>
            <tr>
                <th>Error Type</th>
                <th>Count</th>
                <th>Percentage</th>
            </tr>
"""
            for error_type, count in result.error_distribution.items():
                percentage = (count / result.failed_requests * 100) if result.failed_requests > 0 else 0
                html += f"""
            <tr>
                <td>{error_type}</td>
                <td>{count}</td>
                <td>{percentage:.1f}%</td>
            </tr>
"""
            html += """
        </table>
    </div>
"""
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        if result.bottlenecks:
            html += """
    <div class="section">
        <h2>ğŸ” Detected Bottlenecks</h2>
"""
            for bottleneck in result.bottlenecks:
                html += f"""
        <div class="bottleneck {bottleneck['severity']}">
            <strong>{bottleneck['type'].replace('_', ' ').title()}</strong><br>
            {bottleneck['description']}<br>
            <small>Metric: {bottleneck['metric']} = {bottleneck['value']}</small>
        </div>
"""
            html += """
    </div>
"""
        
        # æ¨å¥¨äº‹é …
        if result.recommendations:
            html += """
    <div class="section">
        <h2>ğŸ’¡ Recommendations</h2>
"""
            for recommendation in result.recommendations:
                html += f"""
        <div class="recommendation">
            {recommendation}
        </div>
"""
            html += """
    </div>
"""
        
        html += """
</body>
</html>
"""
        
        return html
    
    def stop(self):
        """ãƒ†ã‚¹ãƒˆåœæ­¢"""
        self.stop_signal = True
        for user in self.virtual_users:
            user.stop()


# ã‚µãƒ³ãƒ—ãƒ«è² è·ãƒ†ã‚¹ãƒˆé–¢æ•°
async def sample_api_endpoint(payload: Any) -> Dict[str, Any]:
    """ã‚µãƒ³ãƒ—ãƒ«APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    # ãƒ©ãƒ³ãƒ€ãƒ ãªé…å»¶ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    delay = random.uniform(0.1, 0.5)
    await asyncio.sleep(delay)
    
    # ãŸã¾ã«ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
    if random.random() < 0.05:  # 5%ã®ç¢ºç‡ã§ã‚¨ãƒ©ãƒ¼
        raise Exception("Random error occurred")
    
    return {
        'status': 'success',
        'data': payload,
        'timestamp': datetime.now().isoformat()
    }


# ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼é–¢æ•°
def create_load_test_framework(config: Optional[Dict[str, Any]] = None) -> LoadTestFramework:
    """è² è·ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ä½œæˆ"""
    return LoadTestFramework(config)


if __name__ == "__main__":
    # è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¾‹
    async def run_load_tests():
        framework = create_load_test_framework()
        
        # ãƒ†ã‚¹ãƒˆè¨­å®š
        configs = [
            LoadTestConfig(
                name="Constant Load Test",
                description="Test with constant 50 users",
                target_function=sample_api_endpoint,
                load_pattern=LoadPattern.CONSTANT,
                duration=60,
                users=50,
                requests_per_user=20,
                custom_payload={'test': 'data'}
            ),
            LoadTestConfig(
                name="Ramp-up Load Test",
                description="Gradually increase to 100 users",
                target_function=sample_api_endpoint,
                load_pattern=LoadPattern.RAMP_UP,
                duration=120,
                users=100,
                ramp_up_time=60,
                requests_per_user=10
            ),
            LoadTestConfig(
                name="Stress Test",
                description="Find system breaking point",
                target_function=sample_api_endpoint,
                load_pattern=LoadPattern.STRESS,
                duration=300,
                users=500,
                requests_per_user=50
            )
        ]
        
        # å„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        for config in configs:
            print(f"\nğŸš€ Starting: {config.name}")
            print(f"   Pattern: {config.load_pattern.value}")
            print(f"   Users: {config.users}")
            print(f"   Duration: {config.duration}s")
            
            result = await framework.run_load_test(config)
            
            print(f"\nğŸ“Š Results:")
            print(f"   Total Requests: {result.total_requests:,}")
            print(f"   Success Rate: {(1-result.error_rate)*100:.1f}%")
            print(f"   Avg Response Time: {result.average_response_time*1000:.0f}ms")
            print(f"   95th Percentile: {result.percentiles.get('95th', 0)*1000:.0f}ms")
            print(f"   Throughput: {result.throughput/1024/1024:.2f} MB/s")
            
            if result.bottlenecks:
                print(f"\nâš ï¸  Bottlenecks Detected:")
                for bottleneck in result.bottlenecks:
                    print(f"   - {bottleneck['description']}")
            
            print("\n" + "="*50)
    
    # å®Ÿè¡Œ
    asyncio.run(run_load_tests())